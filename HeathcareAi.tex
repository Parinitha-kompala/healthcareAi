% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={HealthCareAI},
  pdfauthor={Parinitha Kompala},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{HealthCareAI}
\author{Parinitha Kompala}
\date{8/26/2021}

\begin{document}
\maketitle

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(healthcareai)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## healthcareai version 2.5.0
## Please visit https://docs.healthcare.ai for full documentation and vignettes. Join the community at https://healthcare-ai.slack.com
\end{verbatim}

analysing the pima data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pima\_diabetes }\CommentTok{\#inbuit dataframe}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 768 x 10
##    patient_id pregnancies plasma_glucose diastolic_bp skinfold insulin
##         <int>       <int>          <int>        <int>    <int>   <int>
##  1          1           6            148           72       35      NA
##  2          2           1             85           66       29      NA
##  3          3           8            183           64       NA      NA
##  4          4           1             89           66       23      94
##  5          5           0            137           40       35     168
##  6          6           5            116           74       NA      NA
##  7          7           3             78           50       32      88
##  8          8          10            115           NA       NA      NA
##  9          9           2            197           70       45     543
## 10         10           8            125           96       NA      NA
## # ... with 758 more rows, and 4 more variables: weight_class <chr>,
## #   pedigree <dbl>, age <int>, diabetes <chr>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(pima\_diabetes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 768  10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(pima\_diabetes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    patient_id     pregnancies     plasma_glucose   diastolic_bp   
##  Min.   :  1.0   Min.   : 0.000   Min.   : 44.0   Min.   : 24.00  
##  1st Qu.:192.8   1st Qu.: 1.000   1st Qu.: 99.0   1st Qu.: 64.00  
##  Median :384.5   Median : 3.000   Median :117.0   Median : 72.00  
##  Mean   :384.5   Mean   : 3.845   Mean   :121.7   Mean   : 72.41  
##  3rd Qu.:576.2   3rd Qu.: 6.000   3rd Qu.:141.0   3rd Qu.: 80.00  
##  Max.   :768.0   Max.   :17.000   Max.   :199.0   Max.   :122.00  
##                                   NA's   :5       NA's   :35      
##     skinfold        insulin       weight_class          pedigree     
##  Min.   : 7.00   Min.   : 14.00   Length:768         Min.   :0.0780  
##  1st Qu.:22.00   1st Qu.: 76.25   Class :character   1st Qu.:0.2437  
##  Median :29.00   Median :125.00   Mode  :character   Median :0.3725  
##  Mean   :29.15   Mean   :155.55                      Mean   :0.4719  
##  3rd Qu.:36.00   3rd Qu.:190.00                      3rd Qu.:0.6262  
##  Max.   :99.00   Max.   :846.00                      Max.   :2.4200  
##  NA's   :227     NA's   :374                                         
##       age          diabetes        
##  Min.   :21.00   Length:768        
##  1st Qu.:24.00   Class :character  
##  Median :29.00   Mode  :character  
##  Mean   :33.24                     
##  3rd Qu.:41.00                     
##  Max.   :81.00                     
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(pima\_diabetes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## tibble [768 x 10] (S3: tbl_df/tbl/data.frame)
##  $ patient_id    : int [1:768] 1 2 3 4 5 6 7 8 9 10 ...
##  $ pregnancies   : int [1:768] 6 1 8 1 0 5 3 10 2 8 ...
##  $ plasma_glucose: int [1:768] 148 85 183 89 137 116 78 115 197 125 ...
##  $ diastolic_bp  : int [1:768] 72 66 64 66 40 74 50 NA 70 96 ...
##  $ skinfold      : int [1:768] 35 29 NA 23 35 NA 32 NA 45 NA ...
##  $ insulin       : int [1:768] NA NA NA 94 168 NA 88 NA 543 NA ...
##  $ weight_class  : chr [1:768] "obese" "overweight" "normal" "overweight" ...
##  $ pedigree      : num [1:768] 0.627 0.351 0.672 0.167 2.288 ...
##  $ age           : int [1:768] 50 31 32 21 33 30 26 29 53 54 ...
##  $ diabetes      : chr [1:768] "Y" "N" "Y" "N" ...
\end{verbatim}

Machine learing models model 1-

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(pima\_diabetes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 10
##   patient_id pregnancies plasma_glucose diastolic_bp skinfold insulin
##        <int>       <int>          <int>        <int>    <int>   <int>
## 1          1           6            148           72       35      NA
## 2          2           1             85           66       29      NA
## 3          3           8            183           64       NA      NA
## 4          4           1             89           66       23      94
## 5          5           0            137           40       35     168
## 6          6           5            116           74       NA      NA
## # ... with 4 more variables: weight_class <chr>, pedigree <dbl>, age <int>,
## #   diabetes <chr>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(pima\_diabetes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "patient_id"     "pregnancies"    "plasma_glucose" "diastolic_bp"  
##  [5] "skinfold"       "insulin"        "weight_class"   "pedigree"      
##  [9] "age"            "diabetes"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quick\_models }\OtherTok{\textless{}{-}} \FunctionTok{machine\_learn}\NormalTok{(pima\_diabetes, patient\_id, }\AttributeTok{outcome =}\NormalTok{ diabetes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Training new data prep recipe...
\end{verbatim}

\begin{verbatim}
## Variable(s) ignored in prep_data won't be used to tune models: patient_id
\end{verbatim}

\begin{verbatim}
## 
## diabetes looks categorical, so training classification algorithms.
\end{verbatim}

\begin{verbatim}
## 
## After data processing, models are being trained on 12 features with 768 observations.
## Based on n_folds = 5 and hyperparameter settings, the following number of models will be trained: 50 rf's, 50 xgb's, and 100 glm's
\end{verbatim}

\begin{verbatim}
## Training with cross validation: Random Forest
\end{verbatim}

\begin{verbatim}
## Training with cross validation: eXtreme Gradient Boosting
\end{verbatim}

\begin{verbatim}
## Training with cross validation: glmnet
\end{verbatim}

\begin{verbatim}
## 
## *** Models successfully trained. The model object contains the training data minus ignored ID columns. ***
## *** If there was PHI in training data, normal PHI protocols apply to the model object. ***
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quick\_models}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Algorithms Trained: Random Forest, eXtreme Gradient Boosting, and glmnet
## Model Name: diabetes
## Target: diabetes
## Class: Classification
## Performance Metric: AUROC
## Number of Observations: 768
## Number of Features: 12
## Models Trained: 2021-08-27 00:25:51 
## 
## Models tuned via 5-fold cross validation over 10 combinations of hyperparameter values.
## Best model: Random Forest
## AUPR = 0.72, AUROC = 0.85
## Optimal hyperparameter values:
##   mtry = 4
##   splitrule = extratrees
##   min.node.size = 19
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(quick\_models)}
\NormalTok{predictions}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## "predicted_diabetes" predicted by Random Forest last trained: 2021-08-27 00:25:51
## Performance in training: AUROC = 0.85
\end{verbatim}

\begin{verbatim}
## # A tibble: 768 x 11
##    diabetes predicted_diabetes patient_id pregnancies plasma_glucose diastolic_bp
##  * <fct>                 <dbl>      <int>       <int>          <int>        <int>
##  1 Y                    0.712           1           6            148           72
##  2 N                    0.130           2           1             85           66
##  3 Y                    0.392           3           8            183           64
##  4 N                    0.0217          4           1             89           66
##  5 Y                    0.539           5           0            137           40
##  6 N                    0.269           6           5            116           74
##  7 Y                    0.0986          7           3             78           50
##  8 N                    0.493           8          10            115           NA
##  9 Y                    0.736           9           2            197           70
## 10 Y                    0.462          10           8            125           96
## # ... with 758 more rows, and 5 more variables: skinfold <int>, insulin <int>,
## #   weight_class <chr>, pedigree <dbl>, age <int>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(predictions)}
\end{Highlighting}
\end{Shaded}

\includegraphics{HeathcareAi_files/figure-latex/unnamed-chunk-3-1.pdf}
plotting the output od prediction 1

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(predictions)}
\end{Highlighting}
\end{Shaded}

\includegraphics{HeathcareAi_files/figure-latex/unnamed-chunk-4-1.pdf}
prediction2-

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_quick\_models }\OtherTok{\textless{}{-}} \FunctionTok{machine\_learn}\NormalTok{(pima\_diabetes, plasma\_glucose, }\AttributeTok{outcome =}\NormalTok{ diabetes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in prep_data(d, !!!dots, outcome = !!outcome, impute = impute): These
## ignored variables have missingness: plasma_glucose
\end{verbatim}

\begin{verbatim}
## Training new data prep recipe...
\end{verbatim}

\begin{verbatim}
## Variable(s) ignored in prep_data won't be used to tune models: plasma_glucose
\end{verbatim}

\begin{verbatim}
## 
## diabetes looks categorical, so training classification algorithms.
\end{verbatim}

\begin{verbatim}
## 
## After data processing, models are being trained on 12 features with 768 observations.
## Based on n_folds = 5 and hyperparameter settings, the following number of models will be trained: 50 rf's, 50 xgb's, and 100 glm's
\end{verbatim}

\begin{verbatim}
## Training with cross validation: Random Forest
\end{verbatim}

\begin{verbatim}
## Training with cross validation: eXtreme Gradient Boosting
\end{verbatim}

\begin{verbatim}
## Training with cross validation: glmnet
\end{verbatim}

\begin{verbatim}
## 
## *** Models successfully trained. The model object contains the training data minus ignored ID columns. ***
## *** If there was PHI in training data, normal PHI protocols apply to the model object. ***
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_quick\_models}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Algorithms Trained: Random Forest, eXtreme Gradient Boosting, and glmnet
## Model Name: diabetes
## Target: diabetes
## Class: Classification
## Performance Metric: AUROC
## Number of Observations: 768
## Number of Features: 12
## Models Trained: 2021-08-27 00:26:08 
## 
## Models tuned via 5-fold cross validation over 10 combinations of hyperparameter values.
## Best model: Random Forest
## AUPR = 0.61, AUROC = 0.78
## Optimal hyperparameter values:
##   mtry = 4
##   splitrule = extratrees
##   min.node.size = 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(quick\_models)}
\NormalTok{a\_predictions}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## "predicted_diabetes" predicted by Random Forest last trained: 2021-08-27 00:25:51
## Performance in training: AUROC = 0.85
\end{verbatim}

\begin{verbatim}
## # A tibble: 768 x 11
##    diabetes predicted_diabetes patient_id pregnancies plasma_glucose diastolic_bp
##  * <fct>                 <dbl>      <int>       <int>          <int>        <int>
##  1 Y                    0.712           1           6            148           72
##  2 N                    0.130           2           1             85           66
##  3 Y                    0.392           3           8            183           64
##  4 N                    0.0217          4           1             89           66
##  5 Y                    0.539           5           0            137           40
##  6 N                    0.269           6           5            116           74
##  7 Y                    0.0986          7           3             78           50
##  8 N                    0.493           8          10            115           NA
##  9 Y                    0.736           9           2            197           70
## 10 Y                    0.462          10           8            125           96
## # ... with 758 more rows, and 5 more variables: skinfold <int>, insulin <int>,
## #   weight_class <chr>, pedigree <dbl>, age <int>
\end{verbatim}

plot of prediction2

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(a\_predictions)}
\end{Highlighting}
\end{Shaded}

\includegraphics{HeathcareAi_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quick\_models }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{predict}\NormalTok{(}\AttributeTok{outcome\_groups =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{HeathcareAi_files/figure-latex/unnamed-chunk-7-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_quick\_models }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{predict}\NormalTok{(}\AttributeTok{outcome\_groups =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{HeathcareAi_files/figure-latex/unnamed-chunk-8-1.pdf}
Doing some data profiling

\begin{verbatim}
missingness(pima_diabetes)
missingness(pima_diabetes) %>%
  plot()
\end{verbatim}

Data preparation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prep\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (d, ..., outcome, recipe = NULL, remove_near_zero_variance = TRUE, 
##     convert_dates = TRUE, impute = TRUE, collapse_rare_factors = TRUE, 
##     PCA = FALSE, center = FALSE, scale = FALSE, make_dummies = TRUE, 
##     add_levels = TRUE, logical_to_numeric = TRUE, factor_outcome = TRUE, 
##     no_prep = FALSE) 
## {
##     if (!is.data.frame(d)) 
##         stop("\"d\" must be a data frame.")
##     orig_data <- d
##     new_recipe <- TRUE
##     if (!is.null(recipe)) {
##         new_recipe <- FALSE
##         recipe <- check_rec_obj(recipe)
##         no_prep <- attr(recipe, "no_prep")
##     }
##     if (no_prep) 
##         remove_near_zero_variance <- convert_dates <- impute <- collapse_rare_factors <- center <- scale <- make_dummies <- add_levels <- logical_to_numeric <- factor_outcome <- FALSE
##     d_missing <- missingness(d, return_df = FALSE)
##     d_ods <- d[0, ]
##     d_levels <- get_factor_levels(d)
##     best_levels <- attr(d, "best_levels")
##     outcome <- rlang::enquo(outcome)
##     remove_outcome <- FALSE
##     ignore_columns <- rlang::quos(...)
##     ignored <- purrr::map_chr(ignore_columns, rlang::quo_name)
##     d_ignore <- NULL
##     if (length(ignored)) {
##         present <- ignored %in% names(d)
##         if (any(!present)) 
##             stop(list_variables(ignored[!present]), " not found in d.")
##         if (length(ignored) >= ncol(d)) 
##             stop("You only have ignored columns. Try again.")
##         d_ignore <- dplyr::select(d, !!ignored)
##         d <- dplyr::select(d, -dplyr::one_of(ignored))
##         m <- missingness(d_ignore) %>% dplyr::filter(percent_missing > 
##             0)
##         if (!purrr::is_empty(m$variable)) 
##             warning("These ignored variables have missingness: ", 
##                 list_variables(m$variable))
##     }
##     opt <- options("contrasts")[[1]][[1]]
##     if (opt != "contr.treatment") {
##         w <- paste0("Your unordered-factor contrasts option is set to ", 
##             opt, ". This may produce unexpected behavior, particularly in make_dummies in prep_data. ", 
##             "Consider resetting it by restarting R, or with: ", 
##             "options(contrasts = c(\"contr.treatment\", \"contr.poly\"))")
##         warning(w)
##     }
##     if (!new_recipe) {
##         message("Prepping data based on provided recipe")
##         newvars <- setdiff(names(d), c(recipe$var_info$variable, 
##             attr(recipe, "ignored_columns")))
##         if (length(newvars)) {
##             warning("These variables were not observed in training ", 
##                 "and will be ignored: ", list_variables(newvars))
##             ignored <- c(ignored, newvars)
##             d_ignore <- dplyr::bind_cols(d_ignore, dplyr::select(d, 
##                 !!newvars))
##         }
##         missing_vars <- setdiff(recipe$var_info$variable[recipe$var_info$role == 
##             "predictor"], names(d))
##         if (length(missing_vars)) 
##             warning("These variables were present in training but are missing or ignored here: ", 
##                 list_variables(missing_vars))
##         if (!is.null(recipe$steps)) {
##             if (attr(recipe$steps[[1]], "class")[1] == "step_nzv") {
##                 if (length(recipe$steps[[1]]$removals)) {
##                   missing_vars <- missing_vars[missing_vars == 
##                     length(recipe$steps[[1]]$removals)]
##                 }
##             }
##         }
##         if (length(missing_vars)) 
##             stop("These variables were present in training but are missing or ignored here: ", 
##                 list_variables(missing_vars))
##         newly_missing <- find_new_missingness(d, recipe)
##         if (length(newly_missing)) 
##             warning("The following variable(s) have missingness that was not present when recipe was trained: ", 
##                 list_variables(newly_missing))
##         outcome_var <- recipe$var_info$variable[recipe$var_info$role == 
##             "outcome"]
##         if (length(outcome_var) && !outcome_var %in% names(d)) 
##             remove_outcome <- TRUE
##     }
##     else {
##         undeclared_ignores <- find_columns_to_ignore(d, c(rlang::quo_name(outcome), 
##             ignored))
##         if (length(undeclared_ignores)) {
##             warning("The following variable(s) look a lot like identifiers: They are ", 
##                 "character-type and have a unique value on every row. They will ", 
##                 "be ignored: ", paste0(undeclared_ignores, collapse = ", "))
##             ignored <- c(ignored, undeclared_ignores)
##             d_ignore <- dplyr::bind_cols(d_ignore, d[, names(d) %in% 
##                 undeclared_ignores, drop = FALSE])
##             d <- d[, !names(d) %in% undeclared_ignores, drop = FALSE]
##         }
##         mes <- "Training new data prep recipe"
##         recipe <- recipes::recipe(d, ~.)
##         recipe$orig_data <- orig_data
##         if (!rlang::quo_is_missing(outcome)) {
##             outcome_name <- rlang::quo_name(outcome)
##             if (!outcome_name %in% names(d)) 
##                 stop(paste(outcome_name, " not found in d."))
##             outcome_vec <- dplyr::pull(d, !!outcome)
##             if (is.logical(outcome_vec) || any(c("TRUE", "FALSE") %in% 
##                 outcome_vec)) 
##                 stop("outcome looks logical. Please convert the outcome to character", 
##                   " with values other than TRUE and FALSE.")
##             if (any(is.na(outcome_vec))) 
##                 stop("Found NA values in the outcome column. Clean your data or ", 
##                   "remove these rows before training a model.")
##             suppressWarnings({
##                 recipe <- recipes::update_role(recipe, !!outcome, 
##                   new_role = "outcome")
##             })
##             if (factor_outcome && all(outcome_vec %in% 0:1)) {
##                 if (!is.numeric(outcome_vec)) 
##                   stop("factor_outcome is TRUE, but ", outcome_name, 
##                     " is a character", "-type variable with 0s and 1s. Consider making it numeric with ", 
##                     "`as.numeric(as.character())")
##                 recipe <- recipe %>% recipes::step_bin2factor(all_outcomes(), 
##                   levels = c("Y", "N"))
##             }
##             mes <- paste0(mes, "...\n")
##         }
##         else {
##             mes <- paste0(mes, " with no outcome variable specified...\n")
##         }
##         message(mes)
##         freq_cut <- 49
##         unique_cut <- 10
##         if (!is.logical(remove_near_zero_variance)) {
##             if (!is.numeric(remove_near_zero_variance)) 
##                 stop("remove_near_zero_variance must be logical or numeric for step_nzv")
##             if (remove_near_zero_variance < 0 | remove_near_zero_variance > 
##                 1) 
##                 stop("remove_near_zero_variance must be numeric between 0 and 1")
##             freq_cut <- remove_near_zero_variance^-1
##             remove_near_zero_variance <- TRUE
##         }
##         if (remove_near_zero_variance) {
##             recipe <- recipe %>% recipes::step_nzv(all_predictors(), 
##                 freq_cut = freq_cut, unique_cut = unique_cut)
##         }
##         prep_check <- recipes::prep(recipe, training = d)
##         removing <- prep_check$steps[[1]]$removals
##         vi <- recipe$var_info
##         nom_preds <- vi$variable[vi$role == "predictor" & vi$type == 
##             "nominal"]
##         if (length(nom_preds) && all(nom_preds %in% removing)) 
##             stop("All your categorical columns will be removed because they have ", 
##                 "near-zero variance, which will break prep_data. ", 
##                 "Be less aggressive in removing near-zero variance columns by ", 
##                 "using a larger value of remove_near_zero_variance or setting it ", 
##                 "to FALSE.\n  ", list_variables(removing))
##         if (!is.character(convert_dates)) {
##             if (!is.logical(convert_dates)) 
##                 stop("convert_dates must be logical, \"none\", \"continuous\", or ", 
##                   "\"categories\"")
##             if (convert_dates) 
##                 convert_dates <- "continuous"
##             else convert_dates <- "none"
##         }
##         if (convert_dates %in% c("continuous", "categories")) {
##             cols <- find_date_cols(d)
##             if (!purrr::is_empty(cols)) {
##                 recipe <- do.call(step_date_hcai, list(recipe = recipe, 
##                   cols, feature_type = convert_dates)) %>% recipes::step_rm(cols)
##             }
##         }
##         else if (convert_dates == "none") {
##             cols <- find_date_cols(d)
##             if (!purrr::is_empty(cols)) 
##                 recipe <- recipes::step_rm(recipe, cols)
##         }
##         else {
##             stop("convert_dates must be logical, \"none\", \"continuous\", or ", 
##                 "\"categories\"")
##         }
##         if (isTRUE(impute)) {
##             recipe <- recipe %>% hcai_impute()
##         }
##         else if (is.list(impute)) {
##             ip <- list(numeric_method = "mean", nominal_method = "new_category", 
##                 numeric_params = NULL, nominal_params = NULL)
##             ip[names(ip) %in% names(impute)] <- impute[names(impute) %in% 
##                 names(ip)]
##             extras <- names(impute)[!(names(impute) %in% names(ip))]
##             if (length(extras > 0)) {
##                 warning("You have extra imputation parameters that won't be used: ", 
##                   list_variables(extras), ". Available params are: ", 
##                   list_variables(names(ip)))
##             }
##             recipe <- recipe %>% hcai_impute(numeric_method = ip$numeric_method, 
##                 nominal_method = ip$nominal_method, numeric_params = ip$numeric_params, 
##                 nominal_params = ip$nominal_params)
##         }
##         else if (impute != FALSE) {
##             stop("impute must be boolean or list.")
##         }
##         if (!(is.numeric(PCA) || is.logical(PCA))) 
##             stop("PCA must be logical or numeric")
##         if (as.logical(PCA)) {
##             if (!(as.logical(center) && as.logical(scale))) {
##                 warning("\"d\" must be centered and scaled to perform PCA. Center and Scale are being set to TRUE.")
##                 center <- as.logical(PCA)
##                 scale <- as.logical(PCA)
##             }
##         }
##         var_info <- recipe$var_info
##         if (any(var_info$type == "numeric" & var_info$role == 
##             "predictor")) {
##             if (isTRUE(as.logical(center))) {
##                 recipe <- recipe %>% recipes::step_center(all_numeric(), 
##                   -all_outcomes())
##             }
##             if (isTRUE(as.logical(scale))) {
##                 recipe <- recipe %>% recipes::step_scale(all_numeric(), 
##                   -all_outcomes())
##             }
##         }
##         if (any(var_info$type == "nominal" & var_info$role == 
##             "predictor")) {
##             if (add_levels) 
##                 recipe <- step_add_levels(recipe, all_nominal(), 
##                   -all_outcomes())
##             if (!is.logical(collapse_rare_factors)) {
##                 if (!is.numeric(collapse_rare_factors)) 
##                   stop("collapse_rare_factors must be logical or numeric")
##                 if (collapse_rare_factors >= 1 || collapse_rare_factors < 
##                   0) 
##                   stop("If numeric, collapse_rare_factors should be between 0 and 1.")
##                 fac_thresh <- collapse_rare_factors
##                 collapse_rare_factors <- TRUE
##             }
##             if (collapse_rare_factors) {
##                 if (!exists("fac_thresh")) 
##                   fac_thresh <- 0.03
##                 recipe <- recipe %>% recipes::step_other(all_nominal(), 
##                   -all_outcomes(), threshold = fac_thresh)
##             }
##             if (add_levels) 
##                 recipe <- step_add_levels(recipe, all_nominal(), 
##                   -all_outcomes())
##             if (isTRUE(make_dummies)) {
##                 make_dummies <- list()
##             }
##             if (is.list(make_dummies)) {
##                 recipe <- recipe %>% step_dummy_hcai(all_nominal(), 
##                   -all_outcomes(), levels = make_dummies)
##             }
##             else if (!is.logical(make_dummies)) {
##                 stop("step_dummies must be logical or list")
##             }
##         }
##         if (as.logical(PCA)) {
##             if (!impute && !is.list(impute)) 
##                 stop("NAs present in \"d\". PCA not compatible when NAs are present.")
##             if (is.logical(PCA)) 
##                 PCA <- 5
##             if (PCA > length(recipes::prep(recipe, training = d)$term_info$role == 
##                 "predictor")) 
##                 stop("Can't have more components than columns in \"d\".")
##             recipe <- recipe %>% recipes::step_pca(all_numeric(), 
##                 -all_outcomes(), num_comp = as.integer(PCA))
##         }
##         recipe <- recipes::prep(recipe, training = d)
##         attr(recipe, "missingness") <- d_missing
##         attr(recipe, "factor_levels") <- d_levels
##     }
##     if (logical_to_numeric) 
##         d <- dplyr::mutate_if(d, is.logical, as.numeric)
##     d <- recipes::bake(recipe, d)
##     steps <- map_chr(recipe$steps, ~attr(.x, "class")[1])
##     if ("step_nzv" %in% steps && length(nzv_removed <- recipe$steps[[which(steps == 
##         "step_nzv")]]$removals)) 
##         message("Removing the following ", length(nzv_removed), 
##             " near-zero variance column(s). ", "If you don't want to remove them, call prep_data with ", 
##             "remove_near_zero_variance as a smaller numeric or FALSE.\n  ", 
##             list_variables(nzv_removed))
##     if (remove_outcome && outcome_var %in% names(d)) 
##         d <- select_not(d, outcome_var)
##     if (rlang::quo_name(outcome) %in% names(d_ods)) 
##         d_ods <- select_not(d_ods, outcome)
##     d <- dplyr::bind_cols(d_ignore, d)
##     if (new_recipe) 
##         recipe$template <- dplyr::bind_cols(d_ignore, recipe$template)
##     attr(recipe, "ignored_columns") <- unname(ignored)
##     attr(recipe, "no_prep") <- no_prep
##     attr(d, "recipe") <- recipe
##     attr(d, "best_levels") <- best_levels
##     attr(d, "original_data_str") <- d_ods
##     d <- tibble::as_tibble(d)
##     class(d) <- c("prepped_df", class(d))
##     return(d)
## }
## <bytecode: 0x7f8cbd358f98>
## <environment: namespace:healthcareai>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{split\_data }\OtherTok{\textless{}{-}} \FunctionTok{split\_train\_test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ pima\_diabetes,}
                               \AttributeTok{outcome =}\NormalTok{ diabetes,}
                               \AttributeTok{p =}\NormalTok{ .}\DecValTok{9}\NormalTok{,}
                               \AttributeTok{seed =} \DecValTok{84105}\NormalTok{)}
\NormalTok{prepped\_training\_data }\OtherTok{\textless{}{-}} \FunctionTok{prep\_data}\NormalTok{(split\_data}\SpecialCharTok{$}\NormalTok{train, patient\_id, }\AttributeTok{outcome =}\NormalTok{ diabetes,}
                                   \AttributeTok{center =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{,}
                                   \AttributeTok{collapse\_rare\_factors =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Training new data prep recipe...
\end{verbatim}

Data training

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models }\OtherTok{\textless{}{-}} \FunctionTok{tune\_models}\NormalTok{(}\AttributeTok{d =}\NormalTok{ prepped\_training\_data,}
                      \AttributeTok{outcome =}\NormalTok{ diabetes,}
                      \AttributeTok{tune\_depth =} \DecValTok{25}\NormalTok{,}
                      \AttributeTok{metric =} \StringTok{"PR"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Variable(s) ignored in prep_data won't be used to tune models: patient_id
\end{verbatim}

\begin{verbatim}
## 
## diabetes looks categorical, so training classification algorithms.
\end{verbatim}

\begin{verbatim}
## 
## After data processing, models are being trained on 13 features with 692 observations.
## Based on n_folds = 5 and hyperparameter settings, the following number of models will be trained: 125 rf's, 125 xgb's, and 250 glm's
\end{verbatim}

\begin{verbatim}
## Training with cross validation: Random Forest
\end{verbatim}

\begin{verbatim}
## Training with cross validation: eXtreme Gradient Boosting
\end{verbatim}

\begin{verbatim}
## Training with cross validation: glmnet
\end{verbatim}

\begin{verbatim}
## 
## *** Models successfully trained. The model object contains the training data minus ignored ID columns. ***
## *** If there was PHI in training data, normal PHI protocols apply to the model object. ***
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{evaluate}\NormalTok{(models, }\AttributeTok{all\_models =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   model                      AUPR AUROC
##   <chr>                     <dbl> <dbl>
## 1 Random Forest             0.703 0.842
## 2 glmnet                    0.688 0.836
## 3 eXtreme Gradient Boosting 0.687 0.820
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models[}\StringTok{"Random Forest"}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = "none"` instead.

## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = "none"` instead.

## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = "none"` instead.

## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = "none"` instead.

## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = "none"` instead.

## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = "none"` instead.
\end{verbatim}

\includegraphics{HeathcareAi_files/figure-latex/unnamed-chunk-10-1.pdf}
faster madel traing

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{untuned\_rf }\OtherTok{\textless{}{-}} \FunctionTok{flash\_models}\NormalTok{(}\AttributeTok{d =}\NormalTok{ prepped\_training\_data,}
                           \AttributeTok{outcome =}\NormalTok{ diabetes,}
                           \AttributeTok{models =} \StringTok{"RF"}\NormalTok{,}
                           \AttributeTok{metric =} \StringTok{"PR"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Variable(s) ignored in prep_data won't be used to tune models: patient_id
\end{verbatim}

\begin{verbatim}
## 
## diabetes looks categorical, so training classification algorithms.
\end{verbatim}

\begin{verbatim}
## 
## After data processing, models are being trained on 13 features with 692 observations.
## Based on n_folds = 5 and hyperparameter settings, the following number of models will be trained: 5 rf's
\end{verbatim}

\begin{verbatim}
## Training at fixed values: Random Forest
\end{verbatim}

\begin{verbatim}
## 
## *** Models successfully trained. The model object contains the training data minus ignored ID columns. ***
## *** If there was PHI in training data, normal PHI protocols apply to the model object. ***
\end{verbatim}

Model interpretations

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{interpret}\NormalTok{(models) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in interpret(models): Interpreting glmnet model, but Random Forest
## performed best in cross-validation and will be used to make predictions. To use
## the glmnet model for predictions, extract it with x['glmnet'].
\end{verbatim}

\includegraphics{HeathcareAi_files/figure-latex/unnamed-chunk-12-1.pdf}
Variable importance

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_variable\_importance}\NormalTok{(models) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{HeathcareAi_files/figure-latex/unnamed-chunk-13-1.pdf}
Exploring the data

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{explore}\NormalTok{(models) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{plot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## With 4 varying features and n_use = 2, using median to aggregate predicted outcomes across age and pregnancies. You could turn `n_use` up to see the impact of more features.
\end{verbatim}

\includegraphics{HeathcareAi_files/figure-latex/unnamed-chunk-14-1.pdf}
Prediction

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(models)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## "predicted_diabetes" predicted by Random Forest last trained: 2021-08-27 00:26:39
## Performance in training: AUPR = 0.7
\end{verbatim}

\begin{verbatim}
## # A tibble: 692 x 11
##    diabetes predicted_diabetes patient_id pregnancies plasma_glucose diastolic_bp
##  * <fct>                 <dbl>      <int>       <int>          <int>        <int>
##  1 Y                    0.691           1           6            148           72
##  2 N                    0.142           2           1             85           66
##  3 Y                    0.432           3           8            183           64
##  4 N                    0.0219          4           1             89           66
##  5 Y                    0.534           5           0            137           40
##  6 N                    0.190           6           5            116           74
##  7 Y                    0.0844          7           3             78           50
##  8 N                    0.398           8          10            115           NA
##  9 Y                    0.776           9           2            197           70
## 10 Y                    0.546          10           8            125           96
## # ... with 682 more rows, and 5 more variables: skinfold <int>, insulin <int>,
## #   weight_class <chr>, pedigree <dbl>, age <int>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test\_predictions }\OtherTok{\textless{}{-}}
  \FunctionTok{predict}\NormalTok{(models,}
\NormalTok{          split\_data}\SpecialCharTok{$}\NormalTok{test,}
          \AttributeTok{risk\_groups =} \FunctionTok{c}\NormalTok{(}\AttributeTok{low =} \DecValTok{30}\NormalTok{, }\AttributeTok{moderate =} \DecValTok{40}\NormalTok{, }\AttributeTok{high =} \DecValTok{20}\NormalTok{, }\AttributeTok{extreme =} \DecValTok{10}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Prepping data based on provided recipe
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(test\_predictions)}
\end{Highlighting}
\end{Shaded}

\includegraphics{HeathcareAi_files/figure-latex/unnamed-chunk-16-1.pdf}
Saving Moving Loading models

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{save\_models}\NormalTok{(models, }\AttributeTok{file =} \StringTok{"my\_models.RDS"}\NormalTok{)}
\NormalTok{models }\OtherTok{\textless{}{-}} \FunctionTok{load\_models}\NormalTok{(}\StringTok{"my\_models.RDS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regression\_models }\OtherTok{\textless{}{-}} \FunctionTok{machine\_learn}\NormalTok{(pima\_diabetes, patient\_id, }\AttributeTok{outcome =}\NormalTok{ age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Training new data prep recipe...
\end{verbatim}

\begin{verbatim}
## Variable(s) ignored in prep_data won't be used to tune models: patient_id
\end{verbatim}

\begin{verbatim}
## 
## age looks numeric, so training regression algorithms.
\end{verbatim}

\begin{verbatim}
## 
## After data processing, models are being trained on 14 features with 768 observations.
## Based on n_folds = 5 and hyperparameter settings, the following number of models will be trained: 50 rf's, 50 xgb's, and 100 glm's
\end{verbatim}

\begin{verbatim}
## Training with cross validation: Random Forest
\end{verbatim}

\begin{verbatim}
## Training with cross validation: eXtreme Gradient Boosting
\end{verbatim}

\begin{verbatim}
## Training with cross validation: glmnet
\end{verbatim}

\begin{verbatim}
## 
## *** Models successfully trained. The model object contains the training data minus ignored ID columns. ***
## *** If there was PHI in training data, normal PHI protocols apply to the model object. ***
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(regression\_models)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Models trained: 2021-08-27 00:26:53
## 
## Models tuned via 5-fold cross validation over 10 combinations of hyperparameter values.
## Best performance: RMSE = 9.1, MAE = 6.5, Rsquared = 0.41
## By Random Forest with hyperparameters:
##   mtry = 4
##   splitrule = variance
##   min.node.size = 17
## 
## Out-of-fold performance of all trained models:
## 
## $`Random Forest`
## # A tibble: 10 x 9
##     mtry splitrule  min.node.size  RMSE Rsquared   MAE RMSESD RsquaredSD MAESD
##    <int> <chr>              <int> <dbl>    <dbl> <dbl>  <dbl>      <dbl> <dbl>
##  1     4 variance              17  9.07    0.410  6.53  0.655     0.0283 0.276
##  2     3 variance               4  9.08    0.412  6.59  0.694     0.0283 0.305
##  3     5 variance               7  9.09    0.404  6.53  0.601     0.0246 0.235
##  4     4 extratrees            17  9.16    0.417  6.66  0.814     0.0408 0.441
##  5     7 variance               2  9.20    0.391  6.62  0.587     0.0209 0.212
##  6    11 variance              18  9.24    0.385  6.59  0.616     0.0309 0.245
##  7     2 extratrees             8  9.98    0.388  7.68  0.806     0.0453 0.465
##  8     2 extratrees            15  9.99    0.388  7.68  0.856     0.0461 0.540
##  9     2 extratrees            14 10.0     0.381  7.71  0.804     0.0414 0.475
## 10     1 variance              10 10.5     0.372  8.38  0.711     0.0277 0.431
## 
## $`eXtreme Gradient Boosting`
## # A tibble: 10 x 13
##        eta max_depth gamma colsample_bytree min_child_weight subsample nrounds
##      <dbl>     <int> <dbl>            <dbl>            <dbl>     <dbl>   <int>
##  1 0.0291          4 5.73             0.730            0.248     0.763     570
##  2 0.176           7 9.76             0.518            3.53      0.744      46
##  3 0.0990          2 0.350            0.624            2.33      0.526     626
##  4 0.423           5 6.79             0.643            3.80      0.940      69
##  5 0.432           5 6.23             0.505           14.2       0.356      30
##  6 0.173           5 2.86             0.849           14.2       0.579     507
##  7 0.412           6 8.83             0.699            0.673     0.657     642
##  8 0.495           2 0.155            0.589            4.22      0.597     278
##  9 0.457           4 0.914            0.556            1.34      0.370      50
## 10 0.00186         3 8.70             0.699            6.46      0.748      38
## # ... with 6 more variables: RMSE <dbl>, Rsquared <dbl>, MAE <dbl>,
## #   RMSESD <dbl>, RsquaredSD <dbl>, MAESD <dbl>
## 
## $glmnet
## # A tibble: 20 x 8
##    alpha  lambda  RMSE Rsquared   MAE RMSESD RsquaredSD MAESD
##    <dbl>   <dbl> <dbl>    <dbl> <dbl>  <dbl>      <dbl> <dbl>
##  1     0 0.00128  9.37    0.377  6.74  0.578     0.0798 0.358
##  2     0 0.00367  9.37    0.377  6.74  0.578     0.0798 0.358
##  3     0 0.00896  9.37    0.377  6.74  0.578     0.0798 0.358
##  4     0 0.0218   9.37    0.377  6.74  0.578     0.0798 0.358
##  5     0 0.0367   9.37    0.377  6.74  0.578     0.0798 0.358
##  6     0 0.0413   9.37    0.377  6.74  0.578     0.0798 0.358
##  7     0 0.210    9.37    0.377  6.74  0.578     0.0798 0.358
##  8     0 0.240    9.37    0.377  6.74  0.578     0.0798 0.358
##  9     0 0.941    9.37    0.377  6.75  0.565     0.0793 0.342
## 10     1 0.0413   9.38    0.376  6.72  0.602     0.0801 0.380
## 11     1 0.0367   9.38    0.376  6.72  0.604     0.0802 0.382
## 12     1 0.0218   9.38    0.376  6.72  0.609     0.0805 0.387
## 13     1 0.00128  9.38    0.376  6.72  0.611     0.0807 0.390
## 14     1 0.00367  9.38    0.376  6.72  0.611     0.0807 0.390
## 15     1 0.00896  9.38    0.376  6.72  0.611     0.0807 0.390
## 16     1 0.210    9.40    0.373  6.73  0.566     0.0770 0.330
## 17     1 0.240    9.40    0.372  6.74  0.563     0.0764 0.323
## 18     1 0.941    9.50    0.366  6.93  0.539     0.0739 0.248
## 19     0 6.89     9.58    0.374  7.12  0.502     0.0725 0.217
## 20     1 6.89    11.7   NaN      9.59  0.549    NA      0.336
\end{verbatim}

Regretion model 2.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_regression\_models }\OtherTok{\textless{}{-}} \FunctionTok{machine\_learn}\NormalTok{(pima\_diabetes, pregnancies, }\AttributeTok{outcome =}\NormalTok{ age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Training new data prep recipe...
\end{verbatim}

\begin{verbatim}
## Variable(s) ignored in prep_data won't be used to tune models: pregnancies
\end{verbatim}

\begin{verbatim}
## 
## age looks numeric, so training regression algorithms.
\end{verbatim}

\begin{verbatim}
## 
## After data processing, models are being trained on 14 features with 768 observations.
## Based on n_folds = 5 and hyperparameter settings, the following number of models will be trained: 50 rf's, 50 xgb's, and 100 glm's
\end{verbatim}

\begin{verbatim}
## Training with cross validation: Random Forest
\end{verbatim}

\begin{verbatim}
## Training with cross validation: eXtreme Gradient Boosting
\end{verbatim}

\begin{verbatim}
## Training with cross validation: glmnet
\end{verbatim}

\begin{verbatim}
## 
## *** Models successfully trained. The model object contains the training data minus ignored ID columns. ***
## *** If there was PHI in training data, normal PHI protocols apply to the model object. ***
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(a\_regression\_models)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Models trained: 2021-08-27 00:27:06
## 
## Models tuned via 5-fold cross validation over 10 combinations of hyperparameter values.
## Best performance: RMSE = 10, MAE = 8.1, Rsquared = 0.22
## By Random Forest with hyperparameters:
##   mtry = 3
##   splitrule = variance
##   min.node.size = 17
## 
## Out-of-fold performance of all trained models:
## 
## $`Random Forest`
## # A tibble: 10 x 9
##     mtry splitrule  min.node.size  RMSE Rsquared   MAE RMSESD RsquaredSD MAESD
##    <int> <chr>              <int> <dbl>    <dbl> <dbl>  <dbl>      <dbl> <dbl>
##  1     3 variance              17  10.4    0.224  8.13  0.803     0.0534 0.408
##  2     5 variance              19  10.4    0.220  8.09  0.770     0.0476 0.403
##  3     4 variance              10  10.4    0.215  8.13  0.773     0.0478 0.411
##  4     5 extratrees             5  10.5    0.201  8.20  0.773     0.0261 0.401
##  5    14 extratrees            10  10.5    0.198  8.18  0.776     0.0248 0.417
##  6    12 variance              15  10.6    0.201  8.17  0.753     0.0498 0.424
##  7    13 variance              14  10.6    0.201  8.17  0.775     0.0504 0.434
##  8     3 extratrees            18  10.6    0.202  8.38  0.823     0.0324 0.354
##  9     1 extratrees            10  11.4    0.149  9.28  0.781     0.0501 0.343
## 10     1 extratrees             8  11.4    0.158  9.28  0.783     0.0516 0.345
## 
## $`eXtreme Gradient Boosting`
## # A tibble: 10 x 13
##       eta max_depth gamma colsample_bytree min_child_weight subsample nrounds
##     <dbl>     <int> <dbl>            <dbl>            <dbl>     <dbl>   <int>
##  1 0.0387         1  7.68            0.810            0.602     0.380     178
##  2 0.294          1  9.57            0.747           13.1       0.438     169
##  3 0.110          2  4.32            0.535           12.4       0.581     629
##  4 0.180          9  5.92            0.721            9.58      0.992      82
##  5 0.169         10  4.56            0.563           14.5       0.766     141
##  6 0.177          4  9.21            0.541            4.34      0.961     398
##  7 0.157          4  7.46            0.704            2.38      0.833     246
##  8 0.482          8  3.13            0.700           23.9       0.382      55
##  9 0.355         10  2.92            0.634            0.271     0.703     458
## 10 0.491          8  4.74            0.623            2.66      0.674     107
## # ... with 6 more variables: RMSE <dbl>, Rsquared <dbl>, MAE <dbl>,
## #   RMSESD <dbl>, RsquaredSD <dbl>, MAESD <dbl>
## 
## $glmnet
## # A tibble: 20 x 8
##    alpha  lambda  RMSE Rsquared   MAE RMSESD RsquaredSD MAESD
##    <dbl>   <dbl> <dbl>    <dbl> <dbl>  <dbl>      <dbl> <dbl>
##  1     1 0.172    10.8    0.171  8.43  0.436     0.0517 0.347
##  2     1 0.372    10.8    0.172  8.45  0.449     0.0544 0.345
##  3     0 1.27     10.8    0.169  8.44  0.411     0.0470 0.326
##  4     0 0.872    10.8    0.169  8.43  0.412     0.0475 0.333
##  5     0 0.826    10.8    0.169  8.43  0.412     0.0475 0.333
##  6     1 0.0449   10.8    0.170  8.42  0.423     0.0493 0.352
##  7     0 0.00549  10.8    0.169  8.42  0.415     0.0479 0.341
##  8     0 0.0124   10.8    0.169  8.42  0.415     0.0479 0.341
##  9     0 0.0156   10.8    0.169  8.42  0.415     0.0479 0.341
## 10     0 0.0449   10.8    0.169  8.42  0.415     0.0479 0.341
## 11     0 0.172    10.8    0.169  8.42  0.415     0.0479 0.341
## 12     0 0.372    10.8    0.169  8.42  0.415     0.0479 0.341
## 13     1 0.0156   10.8    0.169  8.41  0.421     0.0487 0.351
## 14     1 0.0124   10.8    0.169  8.41  0.420     0.0486 0.350
## 15     1 0.00549  10.8    0.169  8.41  0.420     0.0485 0.350
## 16     0 5.36     10.8    0.166  8.54  0.431     0.0427 0.288
## 17     1 0.826    10.8    0.168  8.57  0.454     0.0547 0.312
## 18     1 0.872    10.8    0.167  8.58  0.456     0.0542 0.310
## 19     1 1.27     11.0    0.151  8.74  0.490     0.0456 0.292
## 20     1 5.36     11.7  NaN      9.59  0.579    NA      0.308
\end{verbatim}

\#prediction on a hypothetical new patient 1

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_patient }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{pregnancies =} \DecValTok{0}\NormalTok{,}
  \AttributeTok{plasma\_glucose =} \DecValTok{80}\NormalTok{,}
  \AttributeTok{diastolic\_bp =} \DecValTok{55}\NormalTok{,}
  \AttributeTok{skinfold =} \DecValTok{24}\NormalTok{,}
  \AttributeTok{insulin =} \ConstantTok{NA}\NormalTok{,}
  \AttributeTok{weight\_class =} \StringTok{"???"}\NormalTok{,}
  \AttributeTok{pedigree =}\NormalTok{ .}\DecValTok{2}\NormalTok{,}
  \AttributeTok{diabetes =} \StringTok{"N"}\NormalTok{)}
\FunctionTok{predict}\NormalTok{(regression\_models, new\_patient)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in ready_with_prep(object, newdata, mi): The following variables(s) had the following value(s) in predict that were not observed in training. 
##  weight_class: ???
\end{verbatim}

\begin{verbatim}
## Prepping data based on provided recipe
\end{verbatim}

\begin{verbatim}
## "predicted_age" predicted by Random Forest last trained: 2021-08-27 00:26:53
## Performance in training: RMSE = 9.07
\end{verbatim}

\begin{verbatim}
## # A tibble: 1 x 9
##   predicted_age pregnancies plasma_glucose diastolic_bp skinfold insulin
## *         <dbl>       <dbl>          <dbl>        <dbl>    <dbl> <lgl>  
## 1          23.7           0             80           55       24 NA     
## # ... with 3 more variables: weight_class <chr>, pedigree <dbl>, diabetes <chr>
\end{verbatim}

\#prediction on a hypothetical new patient 2

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_patient\_2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{pregnancies =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{plasma\_glucose =} \DecValTok{130}\NormalTok{,}
  \AttributeTok{diastolic\_bp =} \DecValTok{60}\NormalTok{,}
  \AttributeTok{skinfold =} \DecValTok{24}\NormalTok{,}
  \AttributeTok{insulin =} \ConstantTok{NA}\NormalTok{,}
  \AttributeTok{weight\_class =} \StringTok{"???"}\NormalTok{,}
  \AttributeTok{pedigree =}\NormalTok{ .}\DecValTok{2}\NormalTok{,}
  \AttributeTok{diabetes =} \StringTok{"N"}\NormalTok{)}
\FunctionTok{predict}\NormalTok{(regression\_models, new\_patient\_2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in ready_with_prep(object, newdata, mi): The following variables(s) had the following value(s) in predict that were not observed in training. 
##  weight_class: ???
\end{verbatim}

\begin{verbatim}
## Prepping data based on provided recipe
\end{verbatim}

\begin{verbatim}
## "predicted_age" predicted by Random Forest last trained: 2021-08-27 00:26:53
## Performance in training: RMSE = 9.07
\end{verbatim}

\begin{verbatim}
## # A tibble: 1 x 9
##   predicted_age pregnancies plasma_glucose diastolic_bp skinfold insulin
## *         <dbl>       <dbl>          <dbl>        <dbl>    <dbl> <lgl>  
## 1          24.9           3            130           60       24 NA     
## # ... with 3 more variables: weight_class <chr>, pedigree <dbl>, diabetes <chr>
\end{verbatim}

\end{document}
